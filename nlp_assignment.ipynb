{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Scrapping youtube video's comments using GCP's service - YouTube Data API v3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1...\n",
      "Fetching page 2...\n",
      "Fetching page 3...\n",
      "Fetching page 4...\n",
      "Fetching page 5...\n",
      "Fetching page 6...\n",
      "Fetching page 7...\n",
      "Fetching page 8...\n",
      "Fetching page 9...\n",
      "Total comments fetched: 857\n",
      "Saved to data/youtube_comments.csv\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "VIDEO_ID = \"dc5S4IV_NeA\"   # from the URL\n",
    "MAX_RESULTS = 100          # max per page (YouTube allows up to 100)\n",
    "\n",
    "def build_url(page_token=None):\n",
    "    base = \"https://www.googleapis.com/youtube/v3/commentThreads\"\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"videoId\": VIDEO_ID,\n",
    "        \"key\": API_KEY,\n",
    "        \"textFormat\": \"plainText\",\n",
    "        \"maxResults\": str(MAX_RESULTS),\n",
    "        \"order\": \"time\"       # or \"relevance\"\n",
    "    }\n",
    "    if page_token:\n",
    "        params[\"pageToken\"] = page_token\n",
    "    return base + \"?\" + urllib.parse.urlencode(params)\n",
    "\n",
    "def fetch_page(page_token=None):\n",
    "    url = build_url(page_token)\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        data = response.read().decode(\"utf-8\")\n",
    "    return json.loads(data)\n",
    "\n",
    "def scrape_comments():\n",
    "    all_comments = []\n",
    "    next_page_token = None\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        print(f\"Fetching page {page}...\")\n",
    "        data = fetch_page(next_page_token)\n",
    "\n",
    "        for item in data.get(\"items\", []):\n",
    "            snippet = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "            author = snippet.get(\"authorDisplayName\", \"\")\n",
    "            text = snippet.get(\"textDisplay\", \"\")\n",
    "            published_at = snippet.get(\"publishedAt\", \"\")\n",
    "            like_count = snippet.get(\"likeCount\", 0)\n",
    "\n",
    "            all_comments.append({\n",
    "                \"author\": author,\n",
    "                \"text\": text,\n",
    "                \"published_at\": published_at,\n",
    "                \"likes\": like_count\n",
    "            })\n",
    "\n",
    "        next_page_token = data.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "        # polite pause so we don't hammer the API\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return all_comments\n",
    "\n",
    "def save_to_csv(comments, filename=\"data/youtube_comments.csv\"):\n",
    "    if not comments:\n",
    "        return\n",
    "    fieldnames = [\"author\", \"text\", \"published_at\", \"likes\"]\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in comments:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "comments = scrape_comments()\n",
    "print(f\"Total comments fetched: {len(comments)}\")\n",
    "save_to_csv(comments)\n",
    "print(\"Saved to data/youtube_comments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete. Saved as data/clean_comments.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>published_at</th>\n",
       "      <th>likes</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@guardiannews</td>\n",
       "      <td>John Lewis ad kickstarts Christmas countdown t...</td>\n",
       "      <td>2025-11-04 09:21:26+00:00</td>\n",
       "      <td>40</td>\n",
       "      <td>john lewis ad kickstarts christmas countdown t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Gary-s9r6x</td>\n",
       "      <td>So he hugged his son after years just because ...</td>\n",
       "      <td>2025-11-15 01:01:56+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>so he hugged his son after years just because ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Jess-qy6pm</td>\n",
       "      <td>Was a black singer!!!!!</td>\n",
       "      <td>2025-11-14 22:30:54+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>was a black singer!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@hopeack12345</td>\n",
       "      <td>‚ù§‚ù§‚ù§‚ù§ Came here after watching the diabolical m...</td>\n",
       "      <td>2025-11-14 22:23:44+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>came here after watching the diabolical m&amp;s ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@LessMarxMoreMises</td>\n",
       "      <td>Worst Christmas ad in history! Was it written ...</td>\n",
       "      <td>2025-11-14 21:01:06+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>worst christmas ad in history! was it written ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@rosson1983</td>\n",
       "      <td>Excellent advert. As usual JL blows the other ...</td>\n",
       "      <td>2025-11-14 19:24:21+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>excellent advert. as usual jl blows the other ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@jackieplatts4359</td>\n",
       "      <td>Oh this is lovely and, yes, I‚Äôm crying ü•π</td>\n",
       "      <td>2025-11-14 17:38:26+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>oh this is lovely and, yes, i‚Äôm crying ü•π</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@m535i</td>\n",
       "      <td>Well done to all those who put this amazing Jo...</td>\n",
       "      <td>2025-11-14 16:18:35+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>well done to all those who put this amazing jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@juliabroadley8411</td>\n",
       "      <td>Beautiful üò¢‚ù§</td>\n",
       "      <td>2025-11-14 14:40:39+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>beautiful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@666kismett</td>\n",
       "      <td>Another disappointing Christmas advert</td>\n",
       "      <td>2025-11-14 12:44:17+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>another disappointing christmas advert</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                               text  \\\n",
       "0       @guardiannews  John Lewis ad kickstarts Christmas countdown t...   \n",
       "1         @Gary-s9r6x  So he hugged his son after years just because ...   \n",
       "2         @Jess-qy6pm                            Was a black singer!!!!!   \n",
       "3       @hopeack12345  ‚ù§‚ù§‚ù§‚ù§ Came here after watching the diabolical m...   \n",
       "4  @LessMarxMoreMises  Worst Christmas ad in history! Was it written ...   \n",
       "5         @rosson1983  Excellent advert. As usual JL blows the other ...   \n",
       "6   @jackieplatts4359           Oh this is lovely and, yes, I‚Äôm crying ü•π   \n",
       "7              @m535i  Well done to all those who put this amazing Jo...   \n",
       "8  @juliabroadley8411                                       Beautiful üò¢‚ù§   \n",
       "9         @666kismett             Another disappointing Christmas advert   \n",
       "\n",
       "               published_at  likes  \\\n",
       "0 2025-11-04 09:21:26+00:00     40   \n",
       "1 2025-11-15 01:01:56+00:00      0   \n",
       "2 2025-11-14 22:30:54+00:00      0   \n",
       "3 2025-11-14 22:23:44+00:00      0   \n",
       "4 2025-11-14 21:01:06+00:00      0   \n",
       "5 2025-11-14 19:24:21+00:00      0   \n",
       "6 2025-11-14 17:38:26+00:00      0   \n",
       "7 2025-11-14 16:18:35+00:00      0   \n",
       "8 2025-11-14 14:40:39+00:00      0   \n",
       "9 2025-11-14 12:44:17+00:00      0   \n",
       "\n",
       "                                          clean_text  \n",
       "0  john lewis ad kickstarts christmas countdown t...  \n",
       "1  so he hugged his son after years just because ...  \n",
       "2                            was a black singer!!!!!  \n",
       "3  came here after watching the diabolical m&s ad...  \n",
       "4  worst christmas ad in history! was it written ...  \n",
       "5  excellent advert. as usual jl blows the other ...  \n",
       "6           oh this is lovely and, yes, i‚Äôm crying ü•π  \n",
       "7  well done to all those who put this amazing jo...  \n",
       "8                                          beautiful  \n",
       "9             another disappointing christmas advert  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Load CSV\n",
    "df = pd.read_csv(\"data/youtube_comments.csv\")\n",
    "\n",
    "# ------- CLEANING FUNCTIONS -------\n",
    "\n",
    "# Remove @username\n",
    "def remove_usernames(text):\n",
    "    return re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "# Remove URLs\n",
    "def remove_urls(text):\n",
    "    return re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "# Remove emojis (optional)\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"  \n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        \"\\U00002500-\\U00002BEF\"\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r\"\", text)\n",
    "\n",
    "# Remove extra spaces, quotes, newlines\n",
    "def clean_spacing(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\r\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# ------- APPLY CLEANING -------\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].astype(str)\n",
    "\n",
    "df[\"clean_text\"] = df[\"clean_text\"].apply(remove_usernames)\n",
    "df[\"clean_text\"] = df[\"clean_text\"].apply(remove_urls)\n",
    "df[\"clean_text\"] = df[\"clean_text\"].apply(remove_emojis)\n",
    "df[\"clean_text\"] = df[\"clean_text\"].apply(clean_spacing)\n",
    "df[\"clean_text\"] = df[\"clean_text\"].str.lower()\n",
    "\n",
    "# 2. Drop duplicates\n",
    "df = df.drop_duplicates(subset=\"clean_text\")\n",
    "\n",
    "# 3. Convert timestamp to datetime\n",
    "df[\"published_at\"] = pd.to_datetime(df[\"published_at\"], errors=\"coerce\")\n",
    "\n",
    "# 4. Optionally remove meaningless comments (e.g., only emoji, single words)\n",
    "df = df[df[\"clean_text\"].str.len() > 3]\n",
    "\n",
    "# Save cleaned file\n",
    "df.to_csv(\"data/clean_comments.csv\", index=False)\n",
    "\n",
    "print(\"Cleaning complete. Saved as data/clean_comments.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Topic Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: don | tune | nice | better | absolutely | music | did | love | just | didn | tear | lives\n",
      "Topic 1: beautiful | crying | amazing | advert | jl | oh | real | got | just | great | emotional | doesn\n",
      "Topic 2: aftersun | brilliant | song | worst | guardian | hard | right | seen | feels | miss | hit | got\n",
      "Topic 3: lewis | john | advert | christmas | ad | great | time | best | lovely | brilliant | xmas | adverts\n",
      "Topic 4: best | just | years | christmas | year | love | advert | dads | people | think | son | getting\n",
      "Topic 5: love | family | wow | nailed | tears | white | know | video | bring | dad | finally | wonderful\n",
      "Saved with topics: data/all_comments_with_topics.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "N_TOPICS = 6          # tweak this after you see results\n",
    "N_TOP_WORDS = 12\n",
    "\n",
    "df = pd.read_csv(\"data/clean_comments.csv\")\n",
    "texts = df[\"clean_text\"].fillna(\"\").tolist()\n",
    "\n",
    "# 1) TF-IDF\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_df=0.9,\n",
    "    min_df=5,           # ignore very rare words\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "X = tfidf.fit_transform(texts)\n",
    "\n",
    "# 2) LDA\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=N_TOPICS,\n",
    "    random_state=42,\n",
    "    learning_method=\"batch\"\n",
    ")\n",
    "topic_distributions = lda.fit_transform(X)   # shape: (n_docs, N_TOPICS)\n",
    "\n",
    "# 3) Show top words per topic to help you interpret & label them\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "def print_topics(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_indices = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        print(f\"Topic {topic_idx}: {' | '.join(top_words)}\")\n",
    "\n",
    "print_topics(lda, feature_names, N_TOP_WORDS)\n",
    "\n",
    "# 4) Assign dominant topic per comment\n",
    "df[\"topic_id\"] = topic_distributions.argmax(axis=1)\n",
    "\n",
    "\n",
    "topic_labels = {\n",
    "    0: \"music themes & general praise\",\n",
    "    1: \"strong emotional praise\",\n",
    "    2: \"nostalgia & memories\",\n",
    "    3: \"general christmas advert discussion\",\n",
    "    4: \"mixed evaluations (best vs worst)\",\n",
    "    5: \"family themes & representation\"\n",
    "}\n",
    "df[\"topic_label\"] = df[\"topic_id\"].map(topic_labels)\n",
    "\n",
    "df.to_csv(\"data/all_comments_with_topics.csv\", index=False)\n",
    "print(\"Saved with topics: data/all_comments_with_topics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lasyar/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/lasyar/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved : data/all_comments_with_topics_and_sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "df = pd.read_csv(\"data/all_comments_with_topics.csv\")\n",
    "\n",
    "sentiment_model = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "texts = df[\"clean_text\"].fillna(\"\").tolist()\n",
    "\n",
    "labels = []\n",
    "scores = []\n",
    "\n",
    "# run in small batches so it‚Äôs not super slow\n",
    "BATCH_SIZE = 32\n",
    "for i in range(0, len(texts), BATCH_SIZE):\n",
    "    batch = texts[i:i+BATCH_SIZE]\n",
    "    results = sentiment_model(batch)\n",
    "    for r in results:\n",
    "        labels.append(r[\"label\"])   # POSITIVE / NEGATIVE\n",
    "        scores.append(r[\"score\"])\n",
    "\n",
    "df[\"sentiment_label\"] = labels\n",
    "df[\"sentiment_score\"] = scores\n",
    "\n",
    "df.to_csv(\"data/all_comments_with_topics_and_sentiment.csv\", index=False)\n",
    "print(\"Saved : data/all_comments_with_topics_and_sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Overall Sentiment Distribution ====\n",
      "\n",
      "sentiment_label\n",
      "POSITIVE    66.67\n",
      "NEGATIVE    33.33\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "==== Topic Distribution (Fraction of Comments) ====\n",
      "\n",
      "topic_label\n",
      "general christmas advert discussion    22.88\n",
      "mixed evaluations (best vs worst)      17.47\n",
      "music themes & general praise          15.99\n",
      "nostalgia & memories                   15.50\n",
      "strong emotional praise                14.15\n",
      "family themes & representation         14.02\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "==== Sentiment Within Each Topic ====\n",
      "\n",
      "topic_label                          sentiment_label\n",
      "family themes & representation       POSITIVE           72.81\n",
      "                                     NEGATIVE           27.19\n",
      "general christmas advert discussion  POSITIVE           65.59\n",
      "                                     NEGATIVE           34.41\n",
      "mixed evaluations (best vs worst)    POSITIVE           64.08\n",
      "                                     NEGATIVE           35.92\n",
      "music themes & general praise        POSITIVE           63.85\n",
      "                                     NEGATIVE           36.15\n",
      "nostalgia & memories                 POSITIVE           61.90\n",
      "                                     NEGATIVE           38.10\n",
      "strong emotional praise              POSITIVE           73.91\n",
      "                                     NEGATIVE           26.09\n",
      "Name: percentage, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load the final file with topics + sentiment\n",
    "df = pd.read_csv(\"data/all_comments_with_topics_and_sentiment.csv\")\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Overall Sentiment Summary\n",
    "# ------------------------------\n",
    "print(\"\\n==== Overall Sentiment Distribution ====\\n\")\n",
    "print(df[\"sentiment_label\"].value_counts(normalize=True).apply(lambda x: round(x*100, 2)))\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Topic Distribution\n",
    "# ------------------------------\n",
    "print(\"\\n==== Topic Distribution (Fraction of Comments) ====\\n\")\n",
    "print(df[\"topic_label\"].value_counts(normalize=True).apply(lambda x: round(x*100, 2)))\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Sentiment Within Each Topic\n",
    "# ------------------------------\n",
    "print(\"\\n==== Sentiment Within Each Topic ====\\n\")\n",
    "sentiment_within_topics = (\n",
    "    df.groupby(\"topic_label\")[\"sentiment_label\"]\n",
    "      .value_counts(normalize=True)\n",
    "      .rename(\"percentage\")\n",
    "      .mul(100)\n",
    "      .round(2)\n",
    ")\n",
    "\n",
    "print(sentiment_within_topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
